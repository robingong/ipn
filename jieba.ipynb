{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache d:\\users\\gongha~1\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 5.505 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n",
      "采用,了,动态,规划,查找,最大,概率,路径,,, ,找出,基于,词频,的,最大,切分,组合\n",
      "scikit,-,learn,也,是,由,python,编写,的,机器,学习,算法,库,，,其,实现,了,许多,有用,的,算法,，,对于,文本,分类,来, ,说,，,使用,sklearn,分类,模型,所,需要,的,向量,形式,。,使用,sklearn, ,的, ,naive,_,bayes, ,算法,库, ,可以,快速,构建,一个,朴素,贝叶斯,模型,。\n"
     ]
    }
   ],
   "source": [
    "# https://code.csdn.net/fxsjy/jieba/tree/master\n",
    "\n",
    "\n",
    "#encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print \"Full Mode:\", \"/ \".join(seg_list)  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print \"Default Mode:\", \"/ \".join(seg_list)  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print \", \".join(seg_list)\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print \", \".join(seg_list)\n",
    "\n",
    "seg_list = jieba.cut(\"采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合\")\n",
    "print \",\".join(seg_list)\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(\"scikit-learn也是由python编写的机器学习算法库，其实现了许多有用的算法，对于文本分类来 说，使用sklearn分类模型所需要的向量形式。使用sklearn 的 naive_bayes 算法库 可以快速构建一个朴素贝叶斯模型。\")\n",
    "print \",\".join(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'catedict.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3dcb06e0441a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Load file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'catedict.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mcatedict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcatelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'catedict.csv'"
     ]
    }
   ],
   "source": [
    "# http://myg0u.com/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/2015/05/06/use-sklearn-jieba.html\n",
    "import jieba\n",
    "import csv\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.naive_bayes as nb\n",
    "import sklearn.externals.joblib as jl\n",
    "import sys\n",
    "def predict(txt):\n",
    "    kv = [t for t in jieba.cut(txt)]\n",
    "    mt = fh.transform([kv])\n",
    "    num =  gnb.predict(mt)\n",
    "    for (k,v) in catedict.viewitems():\n",
    "        if(v==num):\n",
    "            print(\" do you mean...%s\" % k )\n",
    "def clipper(txt):\n",
    "    return jieba.cut(txt)\n",
    "if __name__=='__main__':\n",
    "    # Load file.\n",
    "    reader = csv.reader(open('catedict.csv'))\n",
    "    catedict = {}\n",
    "    catelist = []\n",
    "    memolist = []\n",
    "    #The target category. suck as 1 -> \"晚餐\"  , 2 -> \"午餐\"\n",
    "    for i in reader :\n",
    "        catelist +=[  [i[0],int(i[1]) ]  ] \n",
    "    catedict = dict(catelist)\n",
    "    #init vars\n",
    "    #the train data frame.\n",
    "    reader = open('finished.csv','r')\n",
    "    ctr = 0\n",
    "    #that MODEL.\n",
    "    gnb = nb.MultinomialNB(alpha = 0.01)\n",
    "    #Hashing Trick, transfrom dict -> vector\n",
    "    fh = sklearn.feature_extraction.FeatureHasher(n_features=15000,non_negative=True,input_type='string')\n",
    "    kvlist = []\n",
    "    targetlist = []\n",
    "    # use partial fitting because of big data frame.\n",
    "    for col in reader:\n",
    "        line = col.split(',')\n",
    "        if(len(line) == 2):\n",
    "            line[1].replace('\\n','')\n",
    "            kvlist += [  [ i for i in clipper(line[0]) ] ]\n",
    "            targetlist += [int(line[1])]\n",
    "            ctr+=1\n",
    "            sys.stdout.write('\\r' + repr(ctr) + ' rows has been read   ')\n",
    "            sys.stdout.flush()\n",
    "        if(ctr%100000==0):\n",
    "\n",
    "            print(\"\\npartial fitting...\")\n",
    "            X = fh.fit_transform(kvlist)\n",
    "            gnb.partial_fit(X,targetlist,classes = [i for i in catedict.viewvalues()])\n",
    "            # clean context\n",
    "            result = gnb.predict(X)\n",
    "            rate = (result == targetlist).sum()*100.0 / len(result)*1.0\n",
    "            print(\"rate %.2f %% \" % rate)\n",
    "            targetlist = []\n",
    "            kvlist = []\n",
    "\n",
    "    #finally , save the model\n",
    "    jl.dumps(gnb,'final.pkl')\n",
    "    #you can use the model and feature hasher another place to predict text category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('yinong/总卡数和总余额_20160529.csv')\n",
    "print df.shape #(149, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
